optimizer:
  target: torch.optim.Adam
  params:
    lr: 2.0e-5
    weight_decay : 1.0e-4

scheduler:
  target: torch.optim.lr_scheduler.ExponentialLR
  params: 
    gamma: 0.95

criterion:
  target: segmentation_models_pytorch.losses.FocalLoss
  params:
    mode: multiclass
    ignore_index: 3
    gamma: 3

early_stop_params:
  patience: 10
  min_delta: 2.0e-3

batch_size: 32
max_epochs: 500
limit_train_batches: 200
limit_val_batches: 200
train_workers: 8
val_workers: 8
min_val_f1: 0.2
#min_epochs: 15
max_retrain_models: 5
repeat_batches: 1
warmup_epochs: 